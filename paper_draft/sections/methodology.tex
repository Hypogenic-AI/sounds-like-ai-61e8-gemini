\section{Methodology}
\label{sec:methodology}

We aim to identify a linear direction in the residual stream of an LLM that distinguishes between AI-generated and human-written text. Our approach follows the Representation Engineering (\repe) framework, consisting of activation extraction, direction identification, and causal validation via steering.

\subsection{Data Construction}
We utilize a subset of the \raid dataset \citep{liamdugan2024raid}, which contains pairs of human-written and AI-generated texts. For this study, we focus on the scientific abstract domain. We sample 100 human-written scientific abstracts and 100 abstracts generated by \llama-Chat. To ensure consistency and avoid length-based artifacts, all samples are truncated to 512 tokens. A potential confounder noted in the dataset is the frequent use of prefixes like ``In this paper...'' in AI-generated abstracts, which we address during analysis.

\subsection{Activation Extraction}
We use \qwen as our target model. For each of the 200 samples, we perform a forward pass and extract the residual stream activations $\va_l \in \mathbb{R}^d$ across all $L=29$ layers, where $d=1536$ is the model dimension. We collect two types of representations:
\begin{enumerate}[leftmargin=*,itemsep=0pt]
    \item \para{Last Token Activation:} The residual stream state at the final token of the prompt.
    \item \para{Mean-Pooled Activation:} The average of residual stream states across all non-padding tokens in the sequence.
\end{enumerate}

\subsection{Direction Identification}
Following \citet{marks_2023_geometry_truth} and \citet{arditi_2024_refusal_single_direction}, we use the \defn{Difference-in-Means} method to identify the candidate ``AI direction'' $\vv_l$ at each layer $l$:
\begin{equation}
    \vv_l = \frac{1}{|\gD_{AI}|} \sum_{x \in \gD_{AI}} \va_l(x) - \frac{1}{|\gD_{Human}|} \sum_{x \in \gD_{Human}} \va_l(x)
\end{equation}
where $\gD_{AI}$ and $\gD_{Human}$ are the sets of activations for AI-generated and human-written samples, respectively. This vector $\vv_l$ represents the average shift in representation space associated with the AI stylistic quality.

\subsection{Linear Probing}
To evaluate the separability of the two classes, we train logistic regression probes for each layer. We use a 5-fold cross-validation scheme to measure the accuracy of a linear classifier in distinguishing AI from human activations. This serves as a measure of how much ``AI-style'' information is linearly decodable at each stage of the model's processing.

\subsection{Activation Steering}
To prove the causal relevance of the identified direction, we intervene during the generation process. We implement a forward hook at layer $l=14$ (identified as a middle layer where stylistic features are often well-formed). During generation, we modify the residual stream $\va_{14}$ at each token position:
\begin{equation}
    \va'_{14} = \va_{14} + \alpha \cdot \frac{\vv_{14}}{||\vv_{14}||}
\end{equation}
where $\alpha$ is the steering coefficient. We test three conditions: neutral ($\alpha=0$), subtract AI ($\alpha=-5.0$), and add AI ($\alpha=5.0$). We evaluate the generated outputs by feeding them back into the linear probe trained in \secref{sec:methodology}.