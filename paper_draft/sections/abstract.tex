As large language models (LLMs) become increasingly integrated into human workflows, distinguishing between human-written and AI-generated text has become a critical challenge for academic integrity and misinformation mitigation. The Linear Representation Hypothesis (\lrh) suggests that high-level abstract concepts are represented as linear directions in the residual stream of LLMs. In this work, we investigate whether the stylistic quality of being ``AI-generated'' is encoded as a distinct linear feature. Using the \raid dataset and Representation Engineering (\repe) techniques, we identify a ``sounds like AI'' direction in the middle layers of the \qwen model. We find that AI-generated and human-written scientific abstracts are highly linearly separable in the residual stream, even in early layers. Furthermore, we demonstrate that steering activations along this identified direction causally shifts the stylistic quality of the model's output. Specifically, subtracting the identified AI direction from a prompt completion reduces its predicted ``AI-ness'' from 36.2\% to 0.5\%, while adding it increases the probability to 67.1\%. Our findings suggest that LLMs possess an internal stylistic axis representing their own generation patterns, which can be precisely manipulated to control the perceived anthropomorphism of the output.