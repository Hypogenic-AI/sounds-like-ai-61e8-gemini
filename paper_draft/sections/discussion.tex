\section{Discussion}
\label{sec:discussion}

Our results provide strong empirical support for the Linear Representation Hypothesis (\lrh) as applied to the abstract concept of stylistic origin. The fact that a single linear direction can both detect and steer the ``AI-ness'' of a text suggests that LLMs organize their latent space along interpretable axes of variation, even for properties as nuanced as style.

\para{Mechanistic vs. Surface-Level Features.}
A question is whether the identified direction captures deep stylistic patterns or surface-level artifacts. The perfect accuracy at Layer 1 and the identified ``In this paper'' prefix suggest that, for the \raid dataset, the model identifies text origin largely through initial token distributions. However, the successful steering at Layer 14—which shifts complex linguistic traits like tone and vocabulary choice—indicates that this initial identification propagates into a more semantic stylistic representation in the middle layers. This hierarchical encoding, from surface features to stylistic abstractions, is consistent with previous observations of transformer behavior.

\para{Limitations and Robustness.}
This study has several limitations. First, the perfect separability at Layer 1 suggests that our candidate direction may be overfitted to the specific prefix distributions of the \raid dataset. While steering proved effective, the direction might not generalize to human text that mimics AI prefixes or vice versa. Second, our analysis was restricted to a single model (\qwen) and a single domain (scientific abstracts). Further research is needed to determine if the ``AI direction'' is universal across domains (e.g., creative writing, news) and if directions found in one model can detect text generated by another.

\para{Broader Implications.}
The existence of a controllable stylistic axis has significant implications for AI safety and governance. For AI detection, it suggests that internal activation monitoring could be a more reliable signal than external black-box classifiers, as it taps into the model's own internal categorization of its output. For model alignment, it provides a tool for adjusting model persona without retraining, allowing developers to fine-tune the degree of anthropomorphism to suit specific applications.

{\bf How can we find a more robust direction?} Future work should involve adversarial data augmentation—specifically, stripping common prefixes and forcing the model to distinguish between classes based on deeper semantic and syntactic structures. Additionally, exploring the cross-model consistency of these directions would be a crucial step toward a universal theory of AI stylistic representation.