{"title": "Emergent Linear Representations in World Models of Self-Supervised Sequence Models", "year": 2023, "authors": "Neel Nanda, Andrew Lee, Martin Wattenberg", "url": "https://www.semanticscholar.org/paper/bb26227a94ddb2b0088a23e2ec0a170c40bc4d78", "relevance": 3, "abstract": "How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023a). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for \u201cmy colour\u201d vs. \u201copponent\u2019s colour\u201d may be a simple yet powerful way to interpret the model\u2019s internal state. This precise understanding of the internal representations allows us to control the model\u2019s behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.", "citations": 273}
{"title": "Improving Activation Steering in Language Models with Mean-Centring", "year": 2023, "authors": "Ole Jorgensen, Dylan Cope, Nandi Schoots, Murray Shanahan", "url": "https://www.semanticscholar.org/paper/d8f2d0cb5c904acfd7459f9543ebc4f805acbef5", "relevance": 3, "abstract": "Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts.", "citations": 56}
{"title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity", "year": 2024, "authors": "Andrew Lee, Xiaoyan Bai, I. Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea", "url": "https://www.semanticscholar.org/paper/26b2adbe089ea36617c3ec0aa009319929da0550", "relevance": 3, "abstract": "While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.", "citations": 165}
{"title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "year": 2025, "authors": "Charles O'Neill, Slava Chalnev, Chi Chi Zhao, Max Kirkby, Mudith Jayasekara", "url": "https://www.semanticscholar.org/paper/576531260abc89ed3a08a6ce552706e7860e9394", "relevance": 3, "abstract": "Contextual hallucinations -- statements unsupported by given context -- remain a significant challenge in AI. We demonstrate a practical interpretability insight: a generator-agnostic observer model detects hallucinations via a single forward pass and a linear probe on its residual stream. This probe isolates a single, transferable linear direction separating hallucinated from faithful text, outperforming baselines by 5-27 points and showing robust mid-layer performance across Gemma-2 models (2B to 27B). Gradient-times-activation localises this signal to sparse, late-layer MLP activity. Critically, manipulating this direction causally steers generator hallucination rates, proving its actionability. Our results offer novel evidence of internal, low-dimensional hallucination tracking linked to specific MLP sub-circuits, exploitable for detection and mitigation. We release the 2000-example ContraTales benchmark for realistic assessment of such solutions.", "citations": 4}
{"title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets", "year": 2023, "authors": "Samuel Marks, Max Tegmark", "url": "https://www.semanticscholar.org/paper/d59523889679aee15992c4bf6e52b134186d07d3", "relevance": 3, "abstract": "Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs linearly represent the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs.", "citations": 389}
{"title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering", "year": 2025, "authors": "Lucio La Cava, Andrea Tagarelli", "url": "https://api.semanticscholar.org/CorpusId:281675737", "relevance": 3, "abstract": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.", "citations": 0}
{"title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model", "year": 2023, "authors": "Kenneth Li, Oam Patel, Fernanda Vi'egas, H. Pfister, M. Wattenberg", "url": "https://www.semanticscholar.org/paper/405f8f5f1c6df1b3343c812832479aad5180b65f", "relevance": 3, "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the\"truthfulness\"of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.", "citations": 880}
{"title": "Transferring Linear Features Across Language Models With Model Stitching", "year": 2025, "authors": "Alan Chen, Jack Merullo, Alessandro Stolfo, Ellie Pavlick", "url": "https://www.semanticscholar.org/paper/9855a9cca633223a08b27625d36e4b15cd8dfbcc", "relevance": 3, "abstract": "In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. In particular, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.", "citations": 1}
{"title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "year": 2025, "authors": "Giyeong Oh, Woohyun Cho, Siyeol Kim, Suhwan Choi, Younjae Yu", "url": "https://api.semanticscholar.org/CorpusId:278741008", "relevance": 3, "abstract": "Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +3.78 pp top-1 accuracy gain for ViT-B on ImageNet-1k.", "citations": 0}
{"title": "Refusal in Language Models Is Mediated by a Single Direction", "year": 2024, "authors": "Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda", "url": "https://api.semanticscholar.org/CorpusId:270560489", "relevance": 3, "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.", "citations": 465}
{"title": "The Linear Representation Hypothesis and the Geometry of Large Language Models", "year": 2023, "authors": "Kiho Park, Yo Joong Choe, Victor Veitch", "url": "https://www.semanticscholar.org/paper/eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4", "relevance": 3, "abstract": "Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does\"linear representation\"actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of\"linear representation\", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.", "citations": 363}
{"title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models", "year": 2023, "authors": "Hoagy Cunningham, Aidan Ewart, Logan Riggs Smith, R. Huben, Lee Sharkey", "url": "https://www.semanticscholar.org/paper/edb548fe7574d99454b352ffdb61bca93c3072ba", "relevance": 3, "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.", "citations": 865}
{"title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability", "year": 2024, "authors": "ZhongXiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Han Li", "url": "https://www.semanticscholar.org/paper/f3658afcd181e4078e1e96ff86eac224fd92faab", "relevance": 3, "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.", "citations": 68}
{"title": "Convergent Linear Representations of Emergent Misalignment", "year": 2025, "authors": "Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda", "url": "https://www.semanticscholar.org/paper/8b97874ce9904c9f5e712054d24a8b45cc6fed5e", "relevance": 3, "abstract": "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a'misalignment direction'from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.", "citations": 17}
{"title": "Robust LLM safeguarding via refusal feature adversarial training", "year": 2024, "authors": "Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda", "url": "https://www.semanticscholar.org/paper/57b7415577c0b81796de282b6c952e6cc5a6241e", "relevance": 3, "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing LLM safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature. We further show that the operation of refusal feature ablation (RFA) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Refusal Feature Adversarial Training (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.", "citations": 45}
{"title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time", "year": 2025, "authors": "Aviral Gupta, Armaan Sethi, Ameesh Sethi", "url": "https://www.semanticscholar.org/paper/b90b7a35416671541b2ae362999f3c581dcb89fa", "relevance": 3, "abstract": "Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a\"bias vector,\"which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.", "citations": 1}
{"title": "Understanding Reasoning in Thinking Language Models via Steering Vectors", "year": 2025, "authors": "Constantin Venhoff, Iv'an Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda", "url": "https://www.semanticscholar.org/paper/07ed0dc37f6d3d38bd4ee2c738736987fc82b5f8", "relevance": 3, "abstract": "Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.", "citations": 49}
{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "year": 2024, "authors": "Yu Zhao, Xiaotang Du, Giwon Hong, Aryo Pradipta Gema, Alessio Devoto, Hongru Wang, Xuanli He, Kam-Fai Wong, Pasquale Minervini", "url": "https://www.semanticscholar.org/paper/a17ace1a85ccb4be885697766d74e6a13fdd5321", "relevance": 3, "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.", "citations": 8}
{"title": "Fine-Grained control over Music Generation with Activation Steering", "year": 2025, "authors": "Dipanshu Panda, Jayden Koshy Joe, R. HarshithM, Swathi Narashiman, Pranay Mathur, Anish Veerakumar, Aniruddh Krishna, A. Keerthiharan", "url": "https://www.semanticscholar.org/paper/e3851b47a1c153bcadbd1bd96814131d9675a7a1", "relevance": 3, "abstract": "We present a method for fine-grained control over music generation through inference-time interventions on an autoregressive generative music transformer called MusicGen. Our approach enables timbre transfer, style transfer, and genre fusion by steering the residual stream using weights of linear probes trained on it, or by steering the attention layer activations in a similar manner. We observe that modelling this as a regression task provides improved performance, hypothesizing that the mean-squared-error better preserve meaningful directional information in the activation space. Combined with the global conditioning offered by text prompts in MusicGen, our method provides both global and local control over music generation. Audio samples illustrating our method are available at our demo page.", "citations": 0}
{"title": "Computational Basis of LLM's Decision Making in Social Simulation", "year": 2025, "authors": "Ji Ma", "url": "https://api.semanticscholar.org/CorpusId:277824488", "relevance": 3, "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations''(e.g., ``male''to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "citations": 0}
{"title": "An Adversarial Example for Direct Logit Attribution: Memory Management in GELU-4L", "year": 2023, "authors": "James Dao, Yeu-Tong Lau, Can Rager, Jett Janiak", "url": "https://www.semanticscholar.org/paper/ca0a105d631a5cc5873e9e2dfa09c5fdfe81a8e2", "relevance": 3, "abstract": "Prior work suggests that language models manage the limited bandwidth of the residual stream through a \u201cmemory management\u201d mechanism, where certain attention heads and MLP layers clear residual stream directions set by earlier layers. Our study provides concrete evidence for this erasure phenomenon in a 4-layer transformer, identifying heads that consistently remove the output of earlier heads. We further demonstrate that direct logit attribution (DLA), a common technique for interpreting the output of intermediate transformer layers, can show misleading results by not accounting for erasure.", "citations": 5}
{"title": "Jet Expansions of Residual Computation", "year": 2024, "authors": "Yihong Chen, Xiangxiang Xu, Yao Lu, Pontus Stenetorp, Luca Franceschi", "url": "https://api.semanticscholar.org/CorpusId:273228280", "relevance": 3, "abstract": "We introduce a framework for expanding residual computational graphs using jets, operators that generalize truncated Taylor series. Our method provides a systematic approach to disentangle contributions of different computational paths to model predictions. In contrast to existing techniques such as distillation, probing, or early decoding, our expansions rely solely on the model itself and requires no data, training, or sampling from the model. We demonstrate how our framework grounds and subsumes logit lens, reveals a (super-)exponential path structure in the recursive residual depth and opens up several applications. These include sketching a transformer large language model with $n$-gram statistics extracted from its computations, and indexing the models' levels of toxicity knowledge. Our approach enables data-free analysis of residual computation for model interpretability, development, and evaluation.", "citations": 4}
{"title": "Discovering Latent Knowledge in Language Models Without Supervision", "year": 2022, "authors": "Collin Burns, Haotian Ye, D. Klein, J. Steinhardt", "url": "https://www.semanticscholar.org/paper/89c3bd70ad33c4f8832f00ab98872b77861ee0ec", "relevance": 3, "abstract": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.", "citations": 562}
{"title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "year": 2025, "authors": "Zirui He, Mingyu Jin, Bo Shen, Ali Payani, Yongfeng Zhang, Mengnan Du", "url": "https://www.semanticscholar.org/paper/b73ac68566f285ca73981ba7e2ba8f7d2edc761b", "relevance": 3, "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at https://github.com/Ineedanamehere/SAE-SSV.", "citations": 9}
{"title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers", "year": 2025, "authors": "Jingtong Su, Julia Kempe, Karen Ullrich", "url": "https://api.semanticscholar.org/CorpusId:279465297", "relevance": 3, "abstract": "Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing\"safety\"and improve performance on the GSM8K benchmark (+1.6%) by amplifying\"reasoning\". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.", "citations": 3}
{"title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "year": 2025, "authors": "Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe", "url": "https://api.semanticscholar.org/CorpusId:279070746", "relevance": 3, "abstract": "Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.", "citations": 1}
{"title": "Effectively Steer LLM To Follow Preference via Building Confident Directions", "year": 2025, "authors": "Bingqing Song, Boran Han, Shuai Zhang, Hao Wang, Haoyang Fang, Bonan Min, Yuyang Wang, Mingyi Hong", "url": "https://api.semanticscholar.org/CorpusId:276782038", "relevance": 3, "abstract": "Having an LLM that aligns with human preferences is essential for accommodating individual needs, such as maintaining writing style or generating specific topics of interest. The majority of current alignment methods rely on fine-tuning or prompting, which can be either costly or difficult to control. Model steering algorithms, which modify the model output by constructing specific steering directions, are typically easy to implement and optimization-free. However, their capabilities are typically limited to steering the model into one of the two directions (i.e., bidirectional steering), and there has been no theoretical understanding to guarantee their performance. In this work, we propose a theoretical framework to understand and quantify the model steering methods. Inspired by the framework, we propose a confident direction steering method (CONFST) that steers LLMs via modifying their activations at inference time. More specifically, CONFST builds a confident direction that is closely aligned with users' preferences, and this direction is then added to the activations of the LLMs to effectively steer the model output. Our approach offers three key advantages over popular bidirectional model steering methods: 1) It is more powerful, since multiple (i.e. more than two) users' preferences can be aligned simultaneously; 2) It is simple to implement, since there is no need to determine which layer to add the steering vector to; 3) No explicit user instruction is required. We validate our method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks that require shifting the output of LLMs across various topics and styles, achieving superior performance over competing methods.", "citations": 8}
{"title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction", "year": 2025, "authors": "Yihuai Hong, Dian Zhou, Meng Cao, Lei Yu, Zhijing Jin", "url": "https://api.semanticscholar.org/CorpusId:277451917", "relevance": 3, "abstract": "Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.", "citations": 15}
{"title": "Understanding Refusal in Language Models with Sparse Autoencoders", "year": 2025, "authors": "Wei Jie Yeo, Nirmalendu Prakash, Clement Neo, Roy Ka-Wei Lee, Erik Cambria, Ranjan Satapathy", "url": "https://www.semanticscholar.org/paper/6bf7e0506d24a3b3e7c0476ab30128286fc149f5", "relevance": 3, "abstract": "Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify latent features that causally mediate refusal behaviors. We apply our method to two open-source chat models and intervene on refusal-related features to assess their influence on generation, validating their behavioral impact across multiple harmful datasets. This enables a fine-grained inspection of how refusal manifests at the activation level and addresses key research questions such as investigating upstream-downstream latent relationship and understanding the mechanisms of adversarial jailbreaking techniques. We also establish the usefulness of refusal features in enhancing generalization for linear probes to out-of-distribution adversarial samples in classification tasks. We open source our code in https://github.com/wj210/refusal_sae.", "citations": 7}
{"title": "Training Language Models to Explain Their Own Computations", "year": 2025, "authors": "Belinda Z. Li, Zifan Carl Guo, Vincent Huang, Jacob Steinhardt, Jacob Andreas", "url": "https://www.semanticscholar.org/paper/2f967d2b86217368a36511d082ae465de04980c2", "relevance": 3, "abstract": "Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the explainer model is significantly more capable than the target). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods. Code and data at https://github.com/TransluceAI/introspective-interp", "citations": 7}
{"title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "year": 2025, "authors": "Boyi Deng, Yunyang Wan, Yidan Zhang, Baosong Yang, Fuli Feng", "url": "https://www.semanticscholar.org/paper/5af1703757a341a3be991e45ebcea6380787a3ea", "relevance": 3, "abstract": "The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into a sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs. The code is publicly available at https://github.com/Aatrox103/multilingual-llm-features.", "citations": 4}
{"title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models", "year": 2025, "authors": "Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Liu Leqi", "url": "https://www.semanticscholar.org/paper/afc17f1f339b0c93f41ceb05850cb15f168ca2b8", "relevance": 3, "abstract": "It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \\emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \\textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.", "citations": 2}
{"title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement", "year": 2025, "authors": "Anyi Wang, Xuansheng Wu, Dong Shu, Yunpu Ma, Ninghao Liu", "url": "https://www.semanticscholar.org/paper/ae1879c446c17e5707314ef76068a1bd0bd7483c", "relevance": 3, "abstract": "Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs.", "citations": 1}
{"title": "Transformers represent belief state geometry in their residual stream", "year": 2024, "authors": "A. Shai, Sarah E. Marzen, Lucas Teixeira, Alexander Gietelink Oldenziel, P. Riechers", "url": "https://api.semanticscholar.org/CorpusId:270063773", "relevance": 3, "abstract": "What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on. Our work provides a general framework connecting the structure of training data to the geometric structure of activations inside transformers.", "citations": 35}
{"title": "Steering Without Side Effects: Improving Post-Deployment Control of Language Models", "year": 2024, "authors": "Asa Cooper Stickland, Alexander Lyzhov, Jacob Pfau, Salsabila Mahdi, Samuel R. Bowman", "url": "https://www.semanticscholar.org/paper/6e28665fe1c12eaae3670fa41a19e50b09301969", "relevance": 3, "abstract": "Language models (LMs) have been shown to behave unexpectedly post-deployment. For example, new jailbreaks continually arise, allowing model misuse, despite extensive red-teaming and adversarial training from developers. Given most model queries are unproblematic and frequent retraining results in unstable user experience, methods for mitigation of worst-case behavior should be targeted. One such method is classifying inputs as potentially problematic, then selectively applying steering vectors on these problematic inputs, i.e. adding particular vectors to model hidden states. However, steering vectors can also negatively affect model performance, which will be an issue on cases where the classifier was incorrect. We present KL-then-steer (KTS), a technique that decreases the side effects of steering while retaining its benefits, by first training a model to minimize Kullback-Leibler (KL) divergence between a steered and unsteered model on benign inputs, then steering the model that has undergone this training. Our best method prevents 44% of jailbreak attacks compared to the original Llama-2-chat-7B model while maintaining helpfulness (as measured by MT-Bench) on benign requests almost on par with the original LM. To demonstrate the generality and transferability of our method beyond jailbreaks, we show that our KTS model can be steered to reduce bias towards user-suggested answers on TruthfulQA. Code is available: https://github.com/AsaCooperStickland/kl-then-steer.", "citations": 40}
{"title": "Line of Sight: On Linear Representations in VLLMs", "year": 2025, "authors": "Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas, Arthur Conmy", "url": "https://api.semanticscholar.org/CorpusId:279243233", "relevance": 3, "abstract": "Language models can be equipped with multimodal capabilities by fine-tuning on embeddings of visual inputs. But how do such multimodal models represent images in their hidden activations? We explore representations of image concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set of ImageNet classes represented via linearly decodable features in the residual stream. We show that the features are causal by performing targeted edits on the model output. In order to increase the diversity of the studied linear features, we train multimodal Sparse Autoencoders (SAEs), creating a highly interpretable dictionary of text and image features. We find that although model representations across modalities are quite disjoint, they become increasingly shared in deeper layers.", "citations": 3}
{"title": "Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution", "year": 2026, "authors": "Long Zhang, Fangwei Lin", "url": "https://www.semanticscholar.org/paper/605ff7231a81c3424d51ae9b9d8f1d21f017cc5d", "relevance": 3, "abstract": "Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-3-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the\"Manifold Dilution\"hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by\"Orthogonal Interference,\"where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not\"unlearn\"or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.", "citations": 0}
{"title": "The Remarkable Robustness of LLMs: Stages of Inference?", "year": 2024, "authors": "Vedang Lad, Wes Gurnee, Max Tegmark", "url": "https://www.semanticscholar.org/paper/a1533f64b88ba6b9bd5629aaf26256413157aede", "relevance": 1, "abstract": "We investigate the robustness of Large Language Models (LLMs) to structural interventions by deleting and swapping adjacent layers during inference. Surprisingly, models retain 72-95% of their original top-1 prediction accuracy without any fine-tuning. We find that performance degradation is not uniform across layers: interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers. This pattern of localized sensitivity motivates our hypothesis of four stages of inference, observed across diverse model families and sizes: (1) detokenization, where local context is integrated to lift raw token embeddings into higher-level representations; (2) feature engineering, where task- and entity-specific features are iteratively refined; (3) prediction ensembling, where hidden states are aggregated into plausible next-token predictions; and (4) residual sharpening, where irrelevant features are suppressed to finalize the output distribution. Synthesizing behavioral and mechanistic evidence, we provide a framework for interpreting depth-dependent computations in LLMs.", "citations": 95}
{"title": "Sycophancy Hides Linearly in the Attention Heads", "year": 2026, "authors": "R. Genadi, Munachiso Nwadike, Nurdaulet Mukhituly, Hilal AlQuabeh, Tatsuya Hiraoka, Kentaro Inui", "url": "https://www.semanticscholar.org/paper/80abeec76b7a792391231dca2a876344d3ffdac3", "relevance": 1, "abstract": "We find that correct-to-incorrect sycophancy signals are most linearly separable within multi-head attention activations. Motivated by the linear representation hypothesis, we train linear probes across the residual stream, multilayer perceptron (MLP), and attention layers to analyze where these signals emerge. Although separability appears in the residual stream and MLPs, steering using these probes is most effective in a sparse subset of middle-layer attention heads. Using TruthfulQA as the base dataset, we find that probes trained on it transfer effectively to other factual QA benchmarks. Furthermore, comparing our discovered direction to previously identified\"truthful\"directions reveals limited overlap, suggesting that factual accuracy, and deference resistance, arise from related but distinct mechanisms. Attention-pattern analysis further indicates that the influential heads attend disproportionately to expressions of user doubt, contributing to sycophantic shifts. Overall, these findings suggest that sycophancy can be mitigated through simple, targeted linear interventions that exploit the internal geometry of attention activations.", "citations": 1}
{"title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models", "year": 2024, "authors": "Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, Ziyu Yao", "url": "https://www.semanticscholar.org/paper/2ac231b9cff4f5f9054d86c9b540429d4dd687f4", "relevance": 1, "abstract": "Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we provide a comprehensive survey from a task-centric perspective, organizing the taxonomy of MI research around specific research questions or tasks. We outline the fundamental objects of study in MI, along with the techniques, evaluation methods, and key findings for each task in the taxonomy. In particular, we present a task-centric taxonomy as a roadmap for beginners to navigate the field by helping them quickly identify impactful problems in which they are most interested and leverage MI for their benefit. Finally, we discuss the current gaps in the field and suggest potential future directions for MI research.", "citations": 90}
{"title": "Transformer Dynamics: A neuroscientific approach to interpretability of large language models", "year": 2025, "authors": "Jesseba Fernando, Grigori Guitchounts", "url": "https://api.semanticscholar.org/CorpusId:276422389", "relevance": 1, "abstract": "As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractor-like dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a\"neuroscience of AI\"that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.", "citations": 6}
{"title": "Characterizing stable regions in the residual stream of LLMs", "year": 2024, "authors": "Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim", "url": "https://api.semanticscholar.org/CorpusId:272880695", "relevance": 1, "abstract": "We identify stable regions in the residual stream of Transformers, where the model's output remains insensitive to small activation changes, but exhibits high sensitivity at region boundaries. These regions emerge during training and become more defined as training progresses or model size increases. The regions appear to be much larger than previously studied polytopes. Our analysis suggests that these stable regions align with semantic distinctions, where similar prompts cluster within regions, and activations from the same region lead to similar next token predictions. This work provides a promising research direction for understanding the complexity of neural networks, shedding light on training dynamics, and advancing interpretability.", "citations": 2}
{"title": "DiResNet: Direction-Aware Residual Network for Road Extraction in VHR Remote Sensing Images", "year": 2020, "authors": "L. Ding, L. Bruzzone", "url": "https://www.semanticscholar.org/paper/931b0fff09a16630fc06b9c847baa4a621d68215", "relevance": 1, "abstract": "The binary segmentation of roads in very high resolution (VHR) remote sensing images (RSIs) has always been a challenging task due to factors such as occlusions (caused by shadows, trees, buildings, etc.) and the intraclass variances of road surfaces. The wide use of convolutional neural networks (CNNs) has greatly improved the segmentation accuracy and made the task end-to-end trainable. However, there are still margins to improve in terms of the completeness and connectivity of the results. In this article, we consider the specific context of road extraction and present a direction-aware residual network (DiResNet) that includes three main contributions: 1) an asymmetric residual segmentation network with deconvolutional layers and a structural supervision to enhance the learning of road topology (DiResSeg); 2) a pixel-level supervision of local directions to enhance the embedding of linear features; and 3) a refinement network to optimize the segmentation results (DiResRef). Ablation studies on two benchmark data sets (the Massachusetts data set and the DeepGlobe data set) have confirmed the effectiveness of the presented designs. Comparative experiments with other approaches show that the proposed method has advantages in both overall accuracy and F1-score. The code is available at: https://github.com/ggsDing/DiResNet.", "citations": 77}
{"title": "Implicit regularization of deep residual networks towards neural ODEs", "year": 2023, "authors": "P. Marion, Yu-Han Wu, Michael E. Sander, G\u00e9rard Biau", "url": "https://www.semanticscholar.org/paper/5975aca3c006549e7fa57f050854ceb5474c61e0", "relevance": 1, "abstract": "Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to a global minimum. Numerical experiments illustrate our results.", "citations": 21}
{"title": "Improving the Uniformity of the Residual Magnetic Field in the MSR Using Independent Coils Compensation Method", "year": 2025, "authors": "Zhouqiang Yang, Yanbin Li, Xikai Liu, Yupeng Li, Peiling Cui, Shiqiang Zheng, Xinxiu Zhou", "url": "https://www.semanticscholar.org/paper/5c9814c1b36ababf79b704109a2c15834300ee98", "relevance": 1, "abstract": "The Magnetoencephalography (MEG) based on atomic magnetometer has high demand for the uniformity in the measurement area. Due to the door and holes of the magnetically shielding room (MSR), the distribution of residual magnetic field is non-linear, introducing much trouble into the magnetic field compensation. This study proposes a method for improving the uniformity of the residual magnetic field in the compact MSR by injecting different currents into the two coils in each pair of the outside compensation coils. The model and theoretical analysis method have been established, the feasibility has been verified both by the simulation and the experimental test. Results show that the maximum of the three components of the residual magnetic field in the direction of the three axes can be reduced by 18 times, 10.6 times and 8.1 times, respectively, when compared with the results without compensation, and 2.4 times, 2.3 times and 2.2 times, respectively, when compared with the results compensated with the same current. This study can provide certain help for achieving ultra-weak magnetic environment with high uniformity for the MEG measurement. Note to Practitioners\u2014In this article, we focus on the problem of compensating the residual magnetic field in the compact MSR with high efficiency and low cost, which remains the key factor that limits the promotion of the MEG. Method based on the compensation coils in series outside the MSR only deals with the linear or symmetric components, but has nothing to do with the non-linear and asymmetric one caused by the door. We propose a new method for compensating the residual magnetic field based on the coupling between the active and passive shielding, high compensation efficiency and uniformity can be achieved. In future work, we will aim at applying this method to create an ultra-weak magnetic field environment with high uniformity for the MEG measurement.", "citations": 12}
{"title": "Linear Prediction Networks for Residual based Replay Speech Detection", "year": 2025, "authors": "Suresh Veesa, Badugu Vamsi Krishna, Madhusudan Singh", "url": "https://www.semanticscholar.org/paper/156526c7a7c03c9182cbbf3a5e0b5968568a5f2f", "relevance": 1, "abstract": "The linear prediction residual(LPR) component of speech signal plays a crucial role in detecting replay attacks. Replay is the simplest method fraudsters use to deceive automatic speaker verification systems into accepting a fake speaker. The LPR signal conveys information about the excitation source, making it valuable for differentiating between spoofed and authentic speech samples. This work introduces a deep learning classifier called Linear Prediction Network (LPNet), which uses deep residual layers and takes LPR features as input. We explore various source features to evaluate the usefulness of LPR information in replay detection. The source features employed in this study include residual constant Q-cepstral coefficients(RCQCC), Residual mel-frequency cepstral coefficients(RMFCC), and residual phase constant Q cepstral coefficients(RPCQCC). These features serve as the front end for proposed models. Replay speech detection experiments were conducted using standard ASV spoof 2017 version2.0 database. With the LPNet classifier, the source features have provided $14.71 \\%, 21.03 \\%$, and 14.84% equal error rates (EERs), respectively. The score-level combinations of all three source features along with the popular CQCC feature achieved an 8.62% EER. The proposed combination outshines state-of-the-art replay detection techniques, thereby motivating readers to further explore in this direction.", "citations": 0}
{"title": "Residual Echo Suppression Using Dual-Stream Interactive Transformers With Selective Multi-Scale Encoding", "year": 2025, "authors": "Kai Xie, Ziye Yang, Jie Chen, Meng-Qin Zhu", "url": "https://www.semanticscholar.org/paper/80ee1c72896e0b0e1f56b79669a6bc794788db68", "relevance": 1, "abstract": "Traditional acoustic echo cancellation (AEC) employs linear adaptive filters to identify the echo path between the speaker and the microphone. Although AEC significantly enhances audio quality in voice communications, residual echo persists due to factors such as inaccurate estimation of echo path induced by nonlinear distortion of the far-end signal. Consequently, a post-suppression module is essential for achieving sufficient echo attenuation. This paper proposes a novel time domain end-to-end method with selective multi-scale encoder and attentional interactive module for nonlinear residual echo suppression (RES) in double-talk scenarios. Specifically, the selective multi-scale encoder can adaptively assign the channel-wise weights to features with multiple time resolutions based on the changing acoustic environment, thereby regulating the feature stream to effectively recover near-end speech. Moreover, the attentional interactive module provides a novel context-aware fusion strategy. Differing from conventional linear fusion operations, such as addition and concatenation employed in existing RES methods, this module dynamically calculates fusion weights for dual signal streams, enabling the neural network to benefit from their correlations. Experimental results demonstrate the superiority of the proposed method.", "citations": 0}
{"title": "Lossless Compression of Infrared Images via Pixel-Adaptive Prediction and Residual Hierarchical Decomposition", "year": 2026, "authors": "Ya Liu, Zheng Li, Yong Zhang, Rui Zhang", "url": "https://www.semanticscholar.org/paper/7ecf885d0bc2c1394ff9a687fd20a565474a231b", "relevance": 1, "abstract": "Linear array detector-based infrared push-broom imaging systems are widely employed in remote sensing and security surveillance due to their high spatial resolution, wide swath coverage, and low cost. However, the massive data volume generated during continuous scanning presents substantial storage and transmission challenges. To mitigate this issue, we propose a lossless compression algorithm based on pixel-adaptive prediction and hierarchical decomposition of residuals. The algorithm first performs pixel-wise adaptive noise compensation according to local image characteristics and achieves efficient prediction by exploiting the strong inter-pixel correlation along the scanning direction. Subsequently, hierarchical decomposition is applied to high-energy residual blocks to further eliminate spatial redundancy. Finally, the Golomb\u2013Rice coding parameters are adaptively adjusted based on the neighborhood residual energy, optimizing the overall code length distribution. The experimental results demonstrate that our method significantly outperforms most state-of-the-art approaches in terms of both the compression ratio (CR) and bits per pixel (BPP). Moreover, while maintaining a CR comparable to H.265-Intra, our method achieves a 21-fold reduction in time complexity, confirming its superiority for large-format image compression.", "citations": 0}
{"title": "What Layers When: Learning to Skip Compute in LLMs with Residual Gates", "year": 2025, "authors": "Filipe Laitenberger, Dawid J. Kopiczko, Cees G. M. Snoek, Yuki M. Asano", "url": "https://www.semanticscholar.org/paper/6418436f540eaa8f67f58122ea7cf729d01e624d", "relevance": 1, "abstract": "We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by the gate values and skip low-importance ones using a per-layer budget. While early-exit or router-based Mixture-of-Depths models are known to be unstable and need extensive retraining, our smooth, differentiable gates fine-tune stably on top of pretrained models. On long-form reasoning, we save up to 15% compute while retaining over 90% of baseline accuracy. For increasingly larger models, this tradeoff improves drastically. On instruction-tuned models we see accuracy gains at full compute and match baseline quality near 50% savings. The learned gates give insight into transformer information flow (e.g., BOS tokens act as anchors), and the method combines easily with quantization, pruning, and self-speculative decoding.", "citations": 1}
{"title": "Arithmetic Gradient Optimized Deep Residual Path Planning Model for 3D Environment in Unmannered Aerial Vehicles", "year": 2025, "authors": "Vikash Kumar, Seemanti Saha", "url": "https://www.semanticscholar.org/paper/3d32d06dd396b1452f28331022184c3e68e4c178", "relevance": 1, "abstract": "Unmanned Aerial Vehicles (UAVs) are utilized in various applications that necessitate effective path planning strategies. Nevertheless, several algorithms developed recently may not be practical or efficient, especially when dealing with complex, three\u2010dimensional (3D) flight environments. This paper considers real\u2010time path planning based on global and local environmental data using a deep learning approach. For learning the behavior of the UAV state, the obstacle and distance information is trained using the Cascaded Residual Dense Block Network (CRDBN) model. CRDBN offers a solution that preserves both linear and non\u2010linear correlations between state and behavior. Moreover, the hyperparameters of CRDBN are optimized using the Arithmetic Gradient Optimization (AGO) algorithm that ensures precise path planning. AGO makes the network more scalable in the direction of ideal solutions. The tests are carried out using the MATLAB software, and the effectiveness is assessed using metrics related to deep learning as well as efficiency, energy, and accuracy. The proposed method uses 866.73\u2009J of energy while improving the path planning accuracy to 98.32.", "citations": 1}
{"title": "CO-Net: Multi-stream Gaze Estimation Model in Unconstrained Environments", "year": 2025, "authors": "Qiuxia Chen, Chengxin Wang, Ying Tian, Jinfeng Yang", "url": "https://www.semanticscholar.org/paper/3feda0a689fb2141e326714080cf5d20329269a3", "relevance": 1, "abstract": "The eyes are the windows for humans to interact with the outside world and a prerequisite for the promotion of many applications. In gaze estimation, although the full-face image encompasses the eyes, when extracting gaze features, the network often comprehensively considers the information characteristics of the entire face and their interrelationships. This may lead to neglecting the delicate gaze-related information contained in the eye images. To improve the accuracy of gaze direction prediction, this study proposes a novel multi-stream binocular gaze estimation model named CO-Net, which takes facial and binocular images as input. Firstly, we use facial and eye features to preliminarily estimate a primary gaze direction. Considering the differences that arise when observing objects with both eyes in an unconstrained environment, we utilize this characteristic to purify the gaze features. Finally, we use the purified features to linearly optimize the primary gaze direction. Experimental results demonstrate that our method reduces the mean angular error to 3.63\u00b0 on MPIIGaze and 5.9\u00b0 on RT-GENE, representing improvements of 7% and 10%, respectively, over the previous state-of-the-art.", "citations": 0}
{"title": "Data-Driven Quantitative Evaluation of Fault Diagnosability: Integrating Distance and Direction Similarity Metrics", "year": 2024, "authors": "Yang Song, Jiajing Song, Yijia Lu", "url": "https://www.semanticscholar.org/paper/82b92849c9fb5863a3b1a70cecde3bee737e250a", "relevance": 1, "abstract": "This paper presents a novel data-driven approach to fault diagnosability analysis for linear discrete-time systems. Current methods rely heavily on single evaluation functions based on distance similarity, which may not fully utilize directional information in different sets of data. To address this limitation, the paper proposes a comprehensive set of quantitative evaluation indices that consider both distance and direction similarity. This approach introduces data-driven residuals and reformulates the fault diagnosability evaluation as an assessment of the differences between the residuals in fault-free and faulty cases. Specifically, the paper presents detectability measures that provide the relative size of the residual evaluation function caused by faults and noise. Additionally, a set of isolability evaluation indices is proposed by combining distance and direction similarity between the residuals in different faulty cases. These indices can potentially serve as a reference for optimizing system configuration and designing fault diagnosis systems. To verify the effectiveness of the proposed method, we analyze the diagnosability of gas regulators using data collected from an experimental platform. The proposed detectability indices are 0.560, 0.780, and 1.067, while the integrated isolability measures are $4.5\\times 10^{-7}$ , 0.0046, and 0.0044, providing more detailed and precise information than existing methods.", "citations": 1}
{"title": "Block cross-interactive residual smoothing for Lanczos-type solvers for linear systems with multiple right-hand sides", "year": 2024, "authors": "Kensuke Aihara, Akira Imakura, K. Morikuni", "url": "https://www.semanticscholar.org/paper/7ac1ed772eb82f3126c9e092d07d4b6d4ca86c0e", "relevance": 1, "abstract": "Lanczos-type solvers for large sparse linear systems often exhibit large oscillations in the residual norms. In finite precision arithmetic, large oscillations increase the residual gap (the difference between the recursively updated residual and the explicitly computed residual) and a loss of attainable accuracy of the approximations. This issue is addressed using cross-interactive residual smoothing (CIRS). This approach improves convergence behavior and reduces the residual gap. Similar to how the standard Lanczos-type solvers have been extended to global and block versions for solving systems with multiple right-hand sides, CIRS can also be extended to these versions. While we have developed a global CIRS scheme (Gl-CIRS) in our previous study [K. Aihara, A. Imakura, and K. Morikuni, SIAM J. Matrix Anal. Appl., 43 (2022), pp.1308--1330], in this study, we propose a block version (Bl-CIRS). Subsequently, we demonstrate the effectiveness of Bl-CIRS from various perspectives, such as theoretical insights into the convergence behaviors of the residual and approximation norms, numerical experiments on model problems, and a detailed rounding error analysis for the residual gap. For Bl-CIRS, orthonormalizing the columns of direction matrices is crucial in effectively reducing the residual gap. This analysis also complements our previous study and evaluates the residual gap of the block Lanczos-type solvers.", "citations": 0}
{"title": "Residual learning-based two-stream network for RGB-T object tracking", "year": 2022, "authors": "Yili Chen, Minjie Wan, Yunkai Xu, Xiaojie Zhang, Qian Chen, G. Gu", "url": "https://www.semanticscholar.org/paper/8fc89b11842c1b896f94f785d6b50040e0cab767", "relevance": 1, "abstract": "Abstract. RGB-T object tracking is a branch of visual tracking that has been widely applied to many fields, such as intelligent transportation and urban monitoring. Due to the interference of background clutter and occlusion, the existing trackers still suffer from the problems of unreasonable modal fusion strategy, insufficient feature extraction, and loss of semantic information. To solve these problems, we propose a residual learning-based two stream network for RGB-T object tracking. The overall feature extraction network is composed of three branches, and multi-layer convolutions are utilized to extract the features of visible, thermal, and fused images, respectively. First, aiming at improving the effectiveness of feature extraction, a weight generation module for hierarchical feature weight calculation is designed to guide the direction of feature fusion. Then, the residual block is employed to replace the single-layer convolution in order to increase the depth of the network, by which deeper semantic features are learned and the loss of semantic information is alleviated. Finally, a loss function with a penalty term is developed to adjust our network toward the direction of the best tracking performance of the dual modalities. This overcomes the negative impact of poor mode on model training. Experiments implemented on public RGB-T datasets indicate that our algorithm outperforms the recent state-of-the-art trackers in terms of both precision rate and success rate. Compared with the second-best comparison algorithm, our tracker improves the above two metrics by 0.4% and 1.5%, respectively. Our codes are available at https://github.com/MinjieWan/Residual-learning-based-two-stream-network-for-RGB-T-object-tracking.", "citations": 1}
{"title": "SIRCS: Slope-intercept-residual Compression by Correlation Sequencing for Multi-stream High Variation Data", "year": 2019, "authors": "Zixin Ye, Wen Hua, Liwei Wang, Xiaofang Zhou", "url": "https://www.semanticscholar.org/paper/a55892378c1a11b8488bc69747df9c46b7f2bb04", "relevance": 1, "abstract": "", "citations": 0}
